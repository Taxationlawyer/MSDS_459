# Part Three

Named Entity Recognition (NER) is used to link create “relevant” concepts. This NERs are useful in creating knowledge graphs. Ultimately the knowledge graph is used for information extraction (IE). This process has three key ideas that are worth mentioning.

(1)	Although software (including code) makes knowledge extraction practical and efficient, knowledge extraction is not automatic. One reason for this is that language is not universal. For example, there is more than one way to say the same thing. Also, there is slang to contend with (e.g., “that is sick,” can mean that is great, stunning, and so on). Moreover, the information is available in many different languages. Also, the scope of the entity can vary (e.g., John Doe can be an entity—a very narrow and specific entity, and “person” can be an entity—a very broad one). 

(2)	There are many different options that can be used in the NER practice. What is old is new again. Traditional modelling methods can be used for this purpose. These methods include supervised models (i.e., neural networks) and unsupervised algorithms. Interestingly, according to the authors of the book  entitled “Knowledge Graphs,” authored by Kejriwal et al (page 122), hybrid models (a combination of unsupervised and supervised modelling methods) work the best. 

(3)	Interestingly, traditional modelling method adjustments are made in the NER process. One such control involves precision and recall. In traditional modelling methods (e.g., classification trees), increasing the precision output come with a price—a lowering of the recall result. According to the authors, when it comes to NER projects, increasing precision does not have to come with the price of lowering the recall metric. 

In conclusion, what is old is new again. NER methods appear to be very similar to support vector machines. The tuples in the support vector machine output appear to be the same as the entities in results of the NER results. Additionally, many of the traditional modelling methods are used in NER processes (e.g., neural networks). 

**Reference**
* Lane, Hobson and Maria Dyshel. 2025. Natural Language Processing in Action (second edition). Shelter Island, NY: Manning. [ISBN-13: 978-1617299445] Chapter 10, Large language models in the real world, pages 410–469. Available on Course Reserves, and Chapter 11, Information extraction and knowledge graphs, pages 470–512. Available on Course Reserves.
