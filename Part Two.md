# Part Two

Part Two covers domain discovery with the “principal focus … on intelligent and focused crawling.” (page 80, Kejriwal et al) Imagine the task of gathering information from the internet. There were almost 2 billion websites in 2019. (page 80, Kejriwal et al) The many links that are contained in the websites and the lack of uniformity among website formats (along with the enormous amount of information), complicate the goal of extracting useful and relevant information from the “World Wide Web.” Related to focused crawling, there are at least 3 key ideas discussed in Chapters 2 and 3 (of the book entitled “Knowledge Graphs,” by Kejriwal et al): (1) seeds, (2) classification systems for determining relevance, and (3) wrappers. 

As far as using seeds (also called spiders), the web scraping (the gathering of the information from the internet) needs to be started by using a few URLs that contain relevant information (relevant to the user’s task at hand). This process is analogous to using a primer to jump start something (e.g. using an accelerant to start a fire). 

And, related to scraping relevant information, different methods are used to classify information as relevant or not irrelevant. Some of the methods that can be used to determine relevance include K-Means clustering and principal component analysis (PCA). Moreover, hybrid systems are available. The hybrid systems comprise of two classification systems. The first classifier labels the data as relevant or not relevant: and the second classifier (assuming the first classifier labeled the data as relevant) assigns a probability of the likelihood that the data is relevant. 

As previously mentioned, websites are not structurally uniform. For that reason (and other issues), there is no universal wrapper available to scrape the information from the websites. A scrapper is a template that scrapes information using a predetermined setup (format). So, in order for the wrapper to be effective the wrapper’s structure has to match the website’s structure (layout). There are methods that can be used to generate wrappers to make the Information Extraction process more effective. The authors of the book discuss methods of “automatic” wrapper generation methods. 

In conclusion, despite the advances in Information extraction, considerable human experience and intelligence is still required. The practitioner has to oversee the process and determine if the Information Extraction method is gathering relevant information. Moreover, the data miner has to properly and effectively start the Information Extraction process by using effective spiders (seeds). 

**Reference**
*	Lane, Hobson and Maria Dyshel. 2025. Natural Language Processing in Action (second edition). Shelter Island, NY: Manning. [ISBN-13: 978-1617299445] Chapter 10, Large language models in the real world, pages 410–469. Available on Course Reserves, and Chapter 11, Information extraction and knowledge graphs, pages 470–512. Available on Course Reserves.
